# ğŸš€ ç»Ÿä¸€å¤šæ¨¡å‹ LLM SDK - å¿«é€Ÿå¼€å§‹

## âœ… éƒ¨ç½²æ£€æŸ¥æ¸…å•

### 1. ç¯å¢ƒé…ç½®
```bash
# .env.local æ–‡ä»¶é…ç½®
SILICONFLOW_API_KEY=your_siliconflow_key    # å¿…éœ€
GOOGLE_API_KEY=your_google_key              # æ¨è
OPENAI_API_KEY=your_openai_key              # å¯é€‰
```

### 2. æ–‡ä»¶æ£€æŸ¥
ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
- [x] `lib/llm/index.ts` - ç»Ÿä¸€å¯¼å‡º
- [x] `lib/llm/unifiedChat.ts` - æ ¸å¿ƒè°ƒåº¦å™¨
- [x] `lib/llm/providers/gemini.ts` - Gemini æ”¯æŒ
- [x] `lib/llm/providers/openai.ts` - OpenAI æ”¯æŒ
- [x] `lib/llm/providers/siliconFlow.ts` - SiliconFlow æ”¯æŒ
- [x] `lib/llm/config.ts` - é…ç½®ç®¡ç†
- [x] `lib/llm/utils.ts` - å·¥å…·å‡½æ•°

### 3. å¿«é€Ÿæµ‹è¯•
```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
node test-llm-sdk.js

# æˆ–è¿è¡Œå•é¡¹æµ‹è¯•
node test-llm-sdk.js basic
node test-llm-sdk.js smart
```

## ğŸ¯ ç«‹å³ä½¿ç”¨

### æ–¹å¼1: æ™ºèƒ½é€‰æ‹© (æ¨è)
```typescript
import { smartChat } from '@/lib/llm'

// é•¿å†…å®¹ç”Ÿæˆ - è‡ªåŠ¨é€‰æ‹© Gemini 2.0 (å…è´¹)
const response = await smartChat({
  taskType: 'long_generation',
  messages: [
    { role: 'user', content: 'è¯·å†™ä¸€ä»½è¯¦ç»†çš„é¡¹ç›®æ–¹æ¡ˆ' }
  ]
})

console.log(response.content)
```

### æ–¹å¼2: æŒ‡å®šæ¨¡å‹
```typescript
import { unifiedChat } from '@/lib/llm'

// ä½¿ç”¨å…·ä½“æ¨¡å‹
const response = await unifiedChat({
  model: 'gemini-1.5-flash',
  messages: [
    { role: 'user', content: 'ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±' }
  ]
})
```

### æ–¹å¼3: æµå¼å“åº”
```typescript
await unifiedChat({
  model: 'deepseek-ai/DeepSeek-V3',
  messages: [{ role: 'user', content: 'è®²ä¸ªæ•…äº‹' }],
  stream: true,
  onData: (chunk) => {
    process.stdout.write(chunk) // å®æ—¶è¾“å‡º
  },
  onComplete: (content) => {
    console.log('\nå®Œæˆï¼')
  }
})
```

## ğŸ”„ å‡çº§ç°æœ‰ API

### å‡çº§å‰ (å¤æ‚)
```typescript
// éœ€è¦æ‰‹åŠ¨å¤„ç†ä¸åŒæä¾›å•†
if (provider === 'google') {
  // Google ç‰¹å®šé€»è¾‘
} else if (provider === 'siliconflow') {
  // SiliconFlow ç‰¹å®šé€»è¾‘
}
```

### å‡çº§å (ç®€å•)
```typescript
import { smartChat } from '@/lib/llm'

export async function POST(request: NextRequest) {
  const { messages, taskType = 'default' } = await request.json()
  
  const response = await smartChat({
    taskType,
    messages,
    stream: true,
    onData: (chunk) => {
      // å¤„ç†æµå¼æ•°æ®
    }
  })
  
  return new Response(JSON.stringify(response))
}
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä»»åŠ¡ç±»å‹é€‰æ‹©
```typescript
// âœ… æ¨èï¼šæ ¹æ®åœºæ™¯é€‰æ‹©
const taskTypes = {
  'å¿«é€Ÿé—®ç­”': 'fast',
  'é•¿æ–‡ç« å†™ä½œ': 'long_generation', 
  'æ–‡æ¡£åˆ†æ': 'long_context',
  'å›¾åƒç†è§£': 'multimodal',
  'é¢„ç®—æœ‰é™': 'budget',
  'æµ‹è¯•åŠŸèƒ½': 'experimental'
}

await smartChat({ taskType: taskTypes['é•¿æ–‡ç« å†™ä½œ'], messages })
```

### 2. é”™è¯¯å¤„ç†
```typescript
try {
  return await unifiedChat({ model: 'specific-model', messages })
} catch (error) {
  // é™çº§åˆ°æ™ºèƒ½é€‰æ‹©
  console.log('é™çº§åˆ°æ™ºèƒ½é€‰æ‹©...')
  return await smartChat({ taskType: 'default', messages })
}
```

### 3. æˆæœ¬æ§åˆ¶
```typescript
import { estimateRequestCost } from '@/lib/llm/utils'

// ä¼°ç®—æˆæœ¬
const cost = estimateRequestCost('gemini-1.5-flash', 1000, 2000)
if (cost > 0.1) {
  // ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
  return await smartChat({ taskType: 'budget', messages })
}
```

## ğŸ›ï¸ æ¨èæ¨¡å‹é…ç½®

### æ—¥å¸¸ä½¿ç”¨
```typescript
const dailyConfig = {
  å¿«é€Ÿé—®ç­”: 'gemini-1.5-flash',      // å¿«é€Ÿ + ä¾¿å®œ
  é•¿å†…å®¹ç”Ÿæˆ: 'gemini-2.0-flash-exp', // å…è´¹ + é•¿è¾“å‡º
  æ–‡æ¡£åˆ†æ: 'gemini-1.5-pro',        // é•¿ä¸Šä¸‹æ–‡
  ä»£ç ç”Ÿæˆ: 'deepseek-ai/DeepSeek-V3' // ä»£ç èƒ½åŠ›å¼º
}
```

### æˆæœ¬ä¼˜åŒ–
```typescript
const budgetConfig = {
  é¦–é€‰: 'gemini-2.0-flash-exp',  // å®Œå…¨å…è´¹
  å¤‡é€‰: 'gemini-1.5-flash',      // æœ€ä¾¿å®œ
  æœ€å: 'deepseek-ai/DeepSeek-V3' // æ€§ä»·æ¯”é«˜
}
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: API å¯†é’¥é”™è¯¯
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $SILICONFLOW_API_KEY
echo $GOOGLE_API_KEY

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
source .env.local
```

### é—®é¢˜2: æ¨¡å‹ä¸æ”¯æŒ
```typescript
// æ£€æŸ¥å¯ç”¨æ¨¡å‹
import { getAvailableModels } from '@/lib/models'
console.log(getAvailableModels())

// ä½¿ç”¨æ™ºèƒ½é€‰æ‹©ä½œä¸ºå¤‡é€‰
await smartChat({ taskType: 'default', messages })
```

### é—®é¢˜3: ç½‘ç»œè¶…æ—¶
```typescript
// å¢åŠ è¶…æ—¶æ—¶é—´
import { globalLLMConfig } from '@/lib/llm/config'

globalLLMConfig.updateConfig({
  timeout: {
    default: 60000  // 60ç§’
  }
})
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### åŸºç¡€ç›‘æ§
```typescript
const startTime = Date.now()
const response = await smartChat({ taskType, messages })
const duration = Date.now() - startTime

console.log(`è¯·æ±‚è€—æ—¶: ${duration}ms`)
console.log(`ä½¿ç”¨æ¨¡å‹: ${response.model}`)
console.log(`å†…å®¹é•¿åº¦: ${response.content.length}`)
```

### æˆæœ¬è¿½è¸ª
```typescript
import { estimateRequestCost } from '@/lib/llm/utils'

let dailyCost = 0
const cost = estimateRequestCost(modelId, inputLength, outputLength)
dailyCost += cost

console.log(`æœ¬æ¬¡æˆæœ¬: $${cost.toFixed(4)}`)
console.log(`ä»Šæ—¥æ€»æˆæœ¬: $${dailyCost.toFixed(4)}`)
```

## ğŸ‰ å®Œæˆï¼

ç°åœ¨ä½ å·²ç»æ‹¥æœ‰äº†ä¸€ä¸ªå®Œæ•´çš„ç»Ÿä¸€å¤šæ¨¡å‹è°ƒç”¨ç³»ç»Ÿï¼š

1. âœ… **ä¸€å¥—ä»£ç ** - è°ƒç”¨æ‰€æœ‰æ¨¡å‹
2. âœ… **æ™ºèƒ½é€‰æ‹©** - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹  
3. âœ… **æˆæœ¬ä¼˜åŒ–** - ä¼˜å…ˆä½¿ç”¨å…è´¹/ä¾¿å®œæ¨¡å‹
4. âœ… **é”™è¯¯å¤„ç†** - è‡ªåŠ¨é™çº§å’Œé‡è¯•
5. âœ… **æµå¼æ”¯æŒ** - ç»Ÿä¸€çš„æµå¼å“åº”
6. âœ… **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ TypeScript æ”¯æŒ

### ä¸‹ä¸€æ­¥
- åœ¨ä½ çš„ä¸šåŠ¡ä»£ç ä¸­ä½¿ç”¨ `smartChat()` 
- æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µè°ƒæ•´æ¨¡å‹åå¥½
- ç›‘æ§æˆæœ¬å’Œæ€§èƒ½è¡¨ç°
- æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šæ¨¡å‹æ”¯æŒ

ğŸš€ å¼€å§‹äº«å—ç»Ÿä¸€å¤šæ¨¡å‹è°ƒç”¨çš„ä¾¿åˆ©å§ï¼